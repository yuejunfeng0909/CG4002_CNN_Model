{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process dataset\n",
    "Extract data from the txt file for each user. We only extract the data for the time frame where the user is doing one action, not across different actions. Then we segment the data into 2 seconds window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initialized with size: 4429\n",
      "Class 0 has 1161 samples\n",
      "Class 1 has 1078 samples\n",
      "Class 2 has 979 samples\n",
      "Class 3 has 1211 samples\n",
      "shape of data: (6, 100)\n"
     ]
    }
   ],
   "source": [
    "N_classes = 4\n",
    "\n",
    "class MotionDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, data_directory, window_size=100, is_train=True, transform=None):\n",
    "        self.data_directory = data_directory\n",
    "        self.window_size = window_size\n",
    "        self.is_train = is_train\n",
    "        self.dataset_x = []\n",
    "        self.dataset_y = []\n",
    "        self._initialize_dataset()\n",
    "\n",
    "    def _initialize_dataset(self):\n",
    "        raw_labels = open(self.data_directory + \"/labels.txt\", \"r\")\n",
    "        self.dataset_x = []\n",
    "        self.dataset_y = []\n",
    "        current_exp, current_user = 0, 0\n",
    "        acc_current_file_lines = []\n",
    "        gyro_current_file_lines = []\n",
    "        for line in raw_labels:\n",
    "            # get the experiment and user data\n",
    "            line_split = list(map(int,line.split()))\n",
    "\n",
    "            # filter out the transition movements\n",
    "            if line_split[2] > N_classes:\n",
    "                continue\n",
    "\n",
    "            # open new file if the current experiment and user are different from the previous run\n",
    "            if line_split[0] != current_exp or line_split[1] != current_user:\n",
    "                current_exp, current_user = line_split[0], line_split[1]\n",
    "\n",
    "                # Get the accelerometer data\n",
    "                current_file = open(self.data_directory + \"/acc_exp\" + str.zfill(str(current_exp), 2) + \"_user\" + str.zfill(str(current_user), 2) + \".txt\", \"r\")\n",
    "                # print(\"Opened file: \" + current_file.name)\n",
    "                # process lines\n",
    "                acc_current_file_lines = []\n",
    "                for file_line in current_file:\n",
    "                    acc_current_file_lines.append(list(map(float, file_line.split())))\n",
    "                current_file.close()\n",
    "\n",
    "                # Get the gyro data\n",
    "                current_file = open(self.data_directory + \"/gyro_exp\" + str.zfill(str(current_exp), 2) + \"_user\" + str.zfill(str(current_user), 2) + \".txt\", \"r\")\n",
    "                # print(\"Opened file: \" + current_file.name)\n",
    "                gyro_current_file_lines = []\n",
    "                for file_line in current_file:\n",
    "                    gyro_current_file_lines.append(list(map(float, file_line.split())))\n",
    "                current_file.close()\n",
    "            \n",
    "            # get the label, start and end indices\n",
    "            label, start, end = line_split[2:5]\n",
    "            label -= 1 # convert to 0-indexed\n",
    "\n",
    "            # sliding window\n",
    "            for i in range(start, end - window_size, window_size):\n",
    "                # calculate fft for the window\n",
    "                acc_window = acc_current_file_lines[i:i+window_size]\n",
    "                gyro_window = gyro_current_file_lines[i:i+window_size]\n",
    "                inputs = np.concatenate((acc_window, gyro_window), axis=1)\n",
    "                inputs = np.float32(np.transpose(inputs))\n",
    "                self.dataset_x.append(inputs)\n",
    "                self.dataset_y.append(label)\n",
    "\n",
    "        raw_labels.close()\n",
    "\n",
    "        print(\"Dataset initialized with size: \" + str(len(self.dataset_y)))\n",
    "        for i in range(N_classes):\n",
    "            print(\"Class \" + str(i) + \" has \" + str(self.dataset_y.count(i)) + \" samples\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset_x[idx], self.dataset_y[idx]\n",
    "\n",
    "window_size = 100   # 50Hz, 100 samples = 2s of movement\n",
    "dataset = MotionDataset(\"HAPT Data Set/RawData\", window_size)\n",
    "classes = ('WALKING', 'WALKING_UPSTAIRS', 'WALKING_DOWNSTAIRS', 'SITTING', 'STANDING', 'LAYING', 'STAND_TO_SIT', 'SIT_TO_STAND', 'SIT_TO_LIE', 'LIE_TO_SIT', 'STAND_TO_LIE', 'LIE_TO_STAND')[:N_classes]\n",
    "print(\"shape of data: \" + str(dataset[0][0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 100)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample data\n",
    "dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training and testing data\n",
    "batch_size = 32\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/capstone/lib/python3.9/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv1d(6, 64, kernel_size=(6,), stride=(1,))\n",
       "  (1): ReLU()\n",
       "  (2): Conv1d(64, 64, kernel_size=(12,), stride=(1,))\n",
       "  (3): ReLU()\n",
       "  (4): Flatten(start_dim=1, end_dim=-1)\n",
       "  (5): LazyLinear(in_features=0, out_features=128, bias=True)\n",
       "  (6): ReLU()\n",
       "  (7): LazyLinear(in_features=0, out_features=4, bias=True)\n",
       "  (8): Softmax(dim=None)\n",
       ")"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = dataset[0][0].shape\n",
    "print(input_size)\n",
    "\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Conv1d(6, 64, kernel_size=6, stride=1, padding=0),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv1d(64, 64, kernel_size=12, stride=1, padding=0),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Flatten(),\n",
    "    # torch.nn.LazyLinear(64),\n",
    "    # torch.nn.ReLU(),\n",
    "    torch.nn.LazyLinear(128),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.LazyLinear(N_classes),\n",
    "    torch.nn.Softmax()\n",
    ")\n",
    "\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001, amsgrad=True)\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_valid_loss = np.inf\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/capstone/lib/python3.9/site-packages/torch/nn/modules/container.py:204: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 1] \t Training Loss: 0.989951 \t Validation Loss: 0.877772 \t Accuracy: 0.870203\n",
      "evaluation loss reduced, model saved\n",
      "[Epoch: 2] \t Training Loss: 0.854342 \t Validation Loss: 0.859747 \t Accuracy: 0.878104\n",
      "evaluation loss reduced, model saved\n",
      "[Epoch: 3] \t Training Loss: 0.802798 \t Validation Loss: 0.781423 \t Accuracy: 0.965011\n",
      "evaluation loss reduced, model saved\n",
      "[Epoch: 4] \t Training Loss: 0.774071 \t Validation Loss: 0.762951 \t Accuracy: 0.981941\n",
      "evaluation loss reduced, model saved\n",
      "[Epoch: 5] \t Training Loss: 0.770163 \t Validation Loss: 0.778798 \t Accuracy: 0.966140\n",
      "[Epoch: 6] \t Training Loss: 0.756237 \t Validation Loss: 0.755868 \t Accuracy: 0.987585\n",
      "evaluation loss reduced, model saved\n",
      "[Epoch: 7] \t Training Loss: 0.752759 \t Validation Loss: 0.752448 \t Accuracy: 0.992099\n",
      "evaluation loss reduced, model saved\n",
      "[Epoch: 8] \t Training Loss: 0.750424 \t Validation Loss: 0.754429 \t Accuracy: 0.992099\n",
      "[Epoch: 9] \t Training Loss: 0.750176 \t Validation Loss: 0.757601 \t Accuracy: 0.987585\n",
      "[Epoch:10] \t Training Loss: 0.748899 \t Validation Loss: 0.750368 \t Accuracy: 0.994357\n",
      "evaluation loss reduced, model saved\n",
      "[Epoch:11] \t Training Loss: 0.748756 \t Validation Loss: 0.749658 \t Accuracy: 0.995485\n",
      "evaluation loss reduced, model saved\n",
      "[Epoch:12] \t Training Loss: 0.752524 \t Validation Loss: 0.757494 \t Accuracy: 0.986456\n",
      "[Epoch:13] \t Training Loss: 0.750872 \t Validation Loss: 0.752728 \t Accuracy: 0.992099\n",
      "[Epoch:14] \t Training Loss: 0.749637 \t Validation Loss: 0.753907 \t Accuracy: 0.992099\n",
      "[Epoch:15] \t Training Loss: 0.749309 \t Validation Loss: 0.749601 \t Accuracy: 0.995485\n",
      "evaluation loss reduced, model saved\n",
      "[Epoch:16] \t Training Loss: 0.748344 \t Validation Loss: 0.749533 \t Accuracy: 0.993228\n",
      "evaluation loss reduced, model saved\n",
      "[Epoch:17] \t Training Loss: 0.748200 \t Validation Loss: 0.748744 \t Accuracy: 0.995485\n",
      "evaluation loss reduced, model saved\n",
      "Epoch 00017: reducing learning rate of group 0 to 5.0000e-04.\n",
      "[Epoch:18] \t Training Loss: 0.748195 \t Validation Loss: 0.748686 \t Accuracy: 0.995485\n",
      "evaluation loss reduced, model saved\n",
      "[Epoch:19] \t Training Loss: 0.748193 \t Validation Loss: 0.748819 \t Accuracy: 0.995485\n",
      "[Epoch:20] \t Training Loss: 0.748191 \t Validation Loss: 0.748654 \t Accuracy: 0.995485\n",
      "evaluation loss reduced, model saved\n",
      "[Epoch:21] \t Training Loss: 0.748190 \t Validation Loss: 0.748662 \t Accuracy: 0.995485\n",
      "[Epoch:22] \t Training Loss: 0.748189 \t Validation Loss: 0.748656 \t Accuracy: 0.995485\n",
      "[Epoch:23] \t Training Loss: 0.748187 \t Validation Loss: 0.749353 \t Accuracy: 0.995485\n",
      "Epoch 00023: reducing learning rate of group 0 to 2.5000e-04.\n",
      "[Epoch:24] \t Training Loss: 0.748185 \t Validation Loss: 0.748903 \t Accuracy: 0.995485\n",
      "[Epoch:25] \t Training Loss: 0.748183 \t Validation Loss: 0.748895 \t Accuracy: 0.995485\n",
      "[Epoch:26] \t Training Loss: 0.748182 \t Validation Loss: 0.748930 \t Accuracy: 0.995485\n",
      "[Epoch:27] \t Training Loss: 0.748181 \t Validation Loss: 0.748848 \t Accuracy: 0.995485\n",
      "[Epoch:28] \t Training Loss: 0.748181 \t Validation Loss: 0.749295 \t Accuracy: 0.995485\n",
      "[Epoch:29] \t Training Loss: 0.748290 \t Validation Loss: 0.748757 \t Accuracy: 0.995485\n",
      "Epoch 00029: reducing learning rate of group 0 to 1.2500e-04.\n",
      "[Epoch:30] \t Training Loss: 0.748290 \t Validation Loss: 0.748750 \t Accuracy: 0.995485\n",
      "[Epoch:31] \t Training Loss: 0.748179 \t Validation Loss: 0.748722 \t Accuracy: 0.995485\n",
      "[Epoch:32] \t Training Loss: 0.748179 \t Validation Loss: 0.748750 \t Accuracy: 0.995485\n",
      "[Epoch:33] \t Training Loss: 0.748289 \t Validation Loss: 0.748736 \t Accuracy: 0.995485\n",
      "[Epoch:34] \t Training Loss: 0.748179 \t Validation Loss: 0.748747 \t Accuracy: 0.995485\n",
      "[Epoch:35] \t Training Loss: 0.748178 \t Validation Loss: 0.748681 \t Accuracy: 0.995485\n",
      "Epoch 00035: reducing learning rate of group 0 to 6.2500e-05.\n",
      "[Epoch:36] \t Training Loss: 0.748176 \t Validation Loss: 0.748650 \t Accuracy: 0.995485\n",
      "evaluation loss reduced, model saved\n",
      "[Epoch:37] \t Training Loss: 0.748170 \t Validation Loss: 0.748631 \t Accuracy: 0.995485\n",
      "evaluation loss reduced, model saved\n",
      "[Epoch:38] \t Training Loss: 0.748341 \t Validation Loss: 0.749290 \t Accuracy: 0.995485\n",
      "[Epoch:39] \t Training Loss: 0.747911 \t Validation Loss: 0.748652 \t Accuracy: 0.995485\n",
      "[Epoch:40] \t Training Loss: 0.747907 \t Validation Loss: 0.748621 \t Accuracy: 0.995485\n",
      "evaluation loss reduced, model saved\n",
      "[Epoch:41] \t Training Loss: 0.747906 \t Validation Loss: 0.748614 \t Accuracy: 0.995485\n",
      "evaluation loss reduced, model saved\n",
      "Epoch 00041: reducing learning rate of group 0 to 3.1250e-05.\n",
      "[Epoch:42] \t Training Loss: 0.747905 \t Validation Loss: 0.748613 \t Accuracy: 0.995485\n",
      "evaluation loss reduced, model saved\n",
      "[Epoch:43] \t Training Loss: 0.747904 \t Validation Loss: 0.748612 \t Accuracy: 0.995485\n",
      "evaluation loss reduced, model saved\n",
      "[Epoch:44] \t Training Loss: 0.747904 \t Validation Loss: 0.748619 \t Accuracy: 0.995485\n",
      "[Epoch:45] \t Training Loss: 0.747904 \t Validation Loss: 0.748615 \t Accuracy: 0.995485\n",
      "[Epoch:46] \t Training Loss: 0.747903 \t Validation Loss: 0.748621 \t Accuracy: 0.995485\n",
      "[Epoch:47] \t Training Loss: 0.747903 \t Validation Loss: 0.748620 \t Accuracy: 0.995485\n",
      "Epoch 00047: reducing learning rate of group 0 to 1.5625e-05.\n",
      "[Epoch:48] \t Training Loss: 0.747903 \t Validation Loss: 0.748653 \t Accuracy: 0.995485\n",
      "[Epoch:49] \t Training Loss: 0.747902 \t Validation Loss: 0.748626 \t Accuracy: 0.995485\n",
      "[Epoch:50] \t Training Loss: 0.747902 \t Validation Loss: 0.748626 \t Accuracy: 0.995485\n",
      "[Epoch:51] \t Training Loss: 0.747902 \t Validation Loss: 0.748633 \t Accuracy: 0.995485\n",
      "[Epoch:52] \t Training Loss: 0.747902 \t Validation Loss: 0.748629 \t Accuracy: 0.995485\n",
      "[Epoch:53] \t Training Loss: 0.747902 \t Validation Loss: 0.748634 \t Accuracy: 0.995485\n",
      "Epoch 00053: reducing learning rate of group 0 to 7.8125e-06.\n",
      "[Epoch:54] \t Training Loss: 0.747902 \t Validation Loss: 0.748632 \t Accuracy: 0.995485\n",
      "[Epoch:55] \t Training Loss: 0.747902 \t Validation Loss: 0.748631 \t Accuracy: 0.995485\n",
      "[Epoch:56] \t Training Loss: 0.747902 \t Validation Loss: 0.748633 \t Accuracy: 0.995485\n",
      "[Epoch:57] \t Training Loss: 0.748012 \t Validation Loss: 0.748635 \t Accuracy: 0.995485\n",
      "[Epoch:58] \t Training Loss: 0.747902 \t Validation Loss: 0.748728 \t Accuracy: 0.995485\n",
      "[Epoch:59] \t Training Loss: 0.747902 \t Validation Loss: 0.748636 \t Accuracy: 0.995485\n",
      "Epoch 00059: reducing learning rate of group 0 to 3.9063e-06.\n",
      "[Epoch:60] \t Training Loss: 0.747902 \t Validation Loss: 0.748635 \t Accuracy: 0.995485\n",
      "[Epoch:61] \t Training Loss: 0.747901 \t Validation Loss: 0.748636 \t Accuracy: 0.995485\n",
      "[Epoch:62] \t Training Loss: 0.747902 \t Validation Loss: 0.748635 \t Accuracy: 0.995485\n",
      "[Epoch:63] \t Training Loss: 0.747901 \t Validation Loss: 0.748636 \t Accuracy: 0.995485\n",
      "[Epoch:64] \t Training Loss: 0.747901 \t Validation Loss: 0.748636 \t Accuracy: 0.995485\n",
      "[Epoch:65] \t Training Loss: 0.747901 \t Validation Loss: 0.748636 \t Accuracy: 0.995485\n",
      "Epoch 00065: reducing learning rate of group 0 to 1.9531e-06.\n",
      "[Epoch:66] \t Training Loss: 0.747901 \t Validation Loss: 0.749144 \t Accuracy: 0.995485\n",
      "[Epoch:67] \t Training Loss: 0.747901 \t Validation Loss: 0.748638 \t Accuracy: 0.995485\n",
      "[Epoch:68] \t Training Loss: 0.747901 \t Validation Loss: 0.748638 \t Accuracy: 0.995485\n",
      "[Epoch:69] \t Training Loss: 0.747901 \t Validation Loss: 0.748641 \t Accuracy: 0.995485\n",
      "[Epoch:70] \t Training Loss: 0.747901 \t Validation Loss: 0.748637 \t Accuracy: 0.995485\n",
      "[Epoch:71] \t Training Loss: 0.747901 \t Validation Loss: 0.748637 \t Accuracy: 0.995485\n",
      "Epoch 00071: reducing learning rate of group 0 to 9.7656e-07.\n",
      "[Epoch:72] \t Training Loss: 0.747901 \t Validation Loss: 0.748723 \t Accuracy: 0.995485\n",
      "[Epoch:73] \t Training Loss: 0.747901 \t Validation Loss: 0.749146 \t Accuracy: 0.995485\n",
      "[Epoch:74] \t Training Loss: 0.747901 \t Validation Loss: 0.748638 \t Accuracy: 0.995485\n",
      "[Epoch:75] \t Training Loss: 0.747901 \t Validation Loss: 0.749145 \t Accuracy: 0.995485\n",
      "[Epoch:76] \t Training Loss: 0.747901 \t Validation Loss: 0.748638 \t Accuracy: 0.995485\n",
      "[Epoch:77] \t Training Loss: 0.748011 \t Validation Loss: 0.748638 \t Accuracy: 0.995485\n",
      "Epoch 00077: reducing learning rate of group 0 to 4.8828e-07.\n",
      "[Epoch:78] \t Training Loss: 0.747901 \t Validation Loss: 0.748638 \t Accuracy: 0.995485\n",
      "[Epoch:79] \t Training Loss: 0.747901 \t Validation Loss: 0.749175 \t Accuracy: 0.995485\n",
      "[Epoch:80] \t Training Loss: 0.747901 \t Validation Loss: 0.748661 \t Accuracy: 0.995485\n",
      "[Epoch:81] \t Training Loss: 0.747901 \t Validation Loss: 0.748638 \t Accuracy: 0.995485\n",
      "[Epoch:82] \t Training Loss: 0.748011 \t Validation Loss: 0.748661 \t Accuracy: 0.995485\n",
      "[Epoch:83] \t Training Loss: 0.747902 \t Validation Loss: 0.748638 \t Accuracy: 0.995485\n",
      "Epoch 00083: reducing learning rate of group 0 to 2.4414e-07.\n",
      "[Epoch:84] \t Training Loss: 0.747901 \t Validation Loss: 0.748639 \t Accuracy: 0.995485\n",
      "[Epoch:85] \t Training Loss: 0.747901 \t Validation Loss: 0.748639 \t Accuracy: 0.995485\n",
      "[Epoch:86] \t Training Loss: 0.747901 \t Validation Loss: 0.748662 \t Accuracy: 0.995485\n",
      "[Epoch:87] \t Training Loss: 0.747901 \t Validation Loss: 0.748642 \t Accuracy: 0.995485\n",
      "[Epoch:88] \t Training Loss: 0.747901 \t Validation Loss: 0.749152 \t Accuracy: 0.995485\n",
      "[Epoch:89] \t Training Loss: 0.747901 \t Validation Loss: 0.748638 \t Accuracy: 0.995485\n",
      "Epoch 00089: reducing learning rate of group 0 to 1.2207e-07.\n",
      "[Epoch:90] \t Training Loss: 0.747901 \t Validation Loss: 0.748639 \t Accuracy: 0.995485\n",
      "[Epoch:91] \t Training Loss: 0.747901 \t Validation Loss: 0.748639 \t Accuracy: 0.995485\n",
      "[Epoch:92] \t Training Loss: 0.747901 \t Validation Loss: 0.748727 \t Accuracy: 0.995485\n",
      "[Epoch:93] \t Training Loss: 0.747901 \t Validation Loss: 0.748639 \t Accuracy: 0.995485\n",
      "[Epoch:94] \t Training Loss: 0.748011 \t Validation Loss: 0.748639 \t Accuracy: 0.995485\n",
      "[Epoch:95] \t Training Loss: 0.747901 \t Validation Loss: 0.748639 \t Accuracy: 0.995485\n",
      "Epoch 00095: reducing learning rate of group 0 to 6.1035e-08.\n",
      "[Epoch:96] \t Training Loss: 0.747902 \t Validation Loss: 0.748641 \t Accuracy: 0.995485\n",
      "[Epoch:97] \t Training Loss: 0.747902 \t Validation Loss: 0.748640 \t Accuracy: 0.995485\n",
      "[Epoch:98] \t Training Loss: 0.747901 \t Validation Loss: 0.748639 \t Accuracy: 0.995485\n",
      "[Epoch:99] \t Training Loss: 0.747901 \t Validation Loss: 0.748679 \t Accuracy: 0.995485\n",
      "[Epoch:100] \t Training Loss: 0.747901 \t Validation Loss: 0.748639 \t Accuracy: 0.995485\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "net.train()\n",
    "def get_num_correct(preds, labels):\n",
    "    return preds.round().eq(labels).sum().item()\n",
    "\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    total_training_loss = 0.0\n",
    "    batch_count = 0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        batch_count += 1\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        total_training_loss += loss.item()\n",
    "        vaccuracy = 0\n",
    "            \n",
    "    valid_loss = 0.0\n",
    "    net.eval()     # Optional when not using Model Specific layer\n",
    "    vcount = 0\n",
    "    vcorrect = 0\n",
    "    for j, vdata in enumerate(test_loader, 0):\n",
    "        vcount += 1\n",
    "        vdata, vlabels = vdata[0].to(device), vdata[1].to(device)\n",
    "        # Forward Pass\n",
    "        target = net(vdata)\n",
    "        # Find the Loss\n",
    "        loss = criterion(target,vlabels)\n",
    "        # Calculate Loss\n",
    "        valid_loss += loss.item()\n",
    "        # Calculate accuracy\n",
    "        vcorrect += (target.argmax(1) == vlabels).sum().item()\n",
    "    valid_loss = valid_loss / vcount\n",
    "    vaccuracy = vcorrect / len(test_loader.dataset)\n",
    "    print(f'[Epoch:{epoch + 1:2d}] \\t Training Loss: {running_loss / batch_count:5f} \\t Validation Loss: {valid_loss:5f} \\t Accuracy: {vaccuracy:5f}')\n",
    "    \n",
    "    # save model if loss improved, and obtain predictions\n",
    "    if (valid_loss < min_valid_loss):\n",
    "        min_valid_loss = valid_loss\n",
    "        torch.save(net, \"model.pt\")\n",
    "        print(\"evaluation loss reduced, model saved\")\n",
    "    scheduler.step(vaccuracy)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 14.09375, 'True'), Text(32.09374999999999, 0.5, 'Predicted')]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "net = torch.load('model.pt')\n",
    "net.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "for j, vdata in enumerate(test_loader, 0):\n",
    "    vdata, vlabels = vdata[0].to(device), vdata[1].to(device)\n",
    "    output = net(vdata)\n",
    "    y_pred.extend(output.argmax(1).cpu().numpy())\n",
    "    y_true.extend(vlabels.cpu().numpy())\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "hm = sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=classes, yticklabels=classes)\n",
    "hm.set(xlabel=\"True\", ylabel=\"Predicted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('capstone')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f82d4ec2f4e1a7949fc551b5039d8b80c5fc6c2b366144cfac1fa6211cdc80ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
