{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout, Conv1D, MaxPooling1D, Softmax\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.backend import clear_session\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process dataset\n",
    "Extract data from the txt file for each user. We only extract the data for the time frame where the user is doing one action, not across different actions. Then we segment the data into 2 seconds window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file loader\n",
    "from os.path import isfile\n",
    "\n",
    "def load_data(file):\n",
    "    data = pd.read_csv(file, header=None)\n",
    "    return data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 75   # 50Hz, 75 samples = 1.5s of movement\n",
    "window_stride = 25\n",
    "action_types = ('shield', 'reload', 'grenade', 'final')\n",
    "data_depth = 6\n",
    "\n",
    "dataset_users = [0, 1, 2, 3]\n",
    "\n",
    "# Function to load dataset\n",
    "def load_dataset(data_dir):\n",
    "    dataset_x = []\n",
    "    dataset_y = []\n",
    "    \n",
    "    for i in dataset_users:\n",
    "        user_data_dir = f\"{data_dir}/user{i}\"\n",
    "        print(f\"Loading data from {user_data_dir}\")\n",
    "    \n",
    "        for action_type in action_types:\n",
    "\n",
    "            for i in range(1, 100, 1):\n",
    "                # if file exists\n",
    "                if not isfile(user_data_dir + f\"/{action_type}{i}.csv\"):\n",
    "                    # print number of files loaded\n",
    "                    print(f\"Loaded {i-1} {action_type} files\")\n",
    "                    break\n",
    "                data = load_data(user_data_dir + f\"/{action_type}{i}.csv\")\n",
    "                dataset_x.append(np.int32(data))\n",
    "                dataset_y.append(action_types.index(action_type))\n",
    "\n",
    "    print(\"Dataset initialized with size: \" + str(len(dataset_y)))\n",
    "    for i in range(len(action_types)):\n",
    "        print(\"Class \" + str(i) + \" has \" + str(dataset_y.count(i)) + \" samples\")\n",
    "    dataset_y = to_categorical(dataset_y)\n",
    "    return dataset_x, np.array(dataset_y)\n",
    "\n",
    "def load_idle_dataset(data_dir):\n",
    "    dataset_x = []\n",
    "    dataset_y = []\n",
    "    \n",
    "    for i in dataset_users:\n",
    "        user_data_dir = f\"{data_dir}/user{i}\"\n",
    "        print(f\"Loading idle data from {user_data_dir}\")\n",
    "    \n",
    "        for i in range(1, 100, 1):\n",
    "            # if file exists\n",
    "            if not isfile(user_data_dir + f\"/idle{i}.csv\"):\n",
    "                break\n",
    "            data = load_data(user_data_dir + f\"/idle{i}.csv\")\n",
    "            dataset_x.append(data)\n",
    "            dataset_y.append([0.25, 0.25, 0.25, 0.25])\n",
    "\n",
    "    print(\"Idle dataset initialized with size: \" + str(len(dataset_y)))\n",
    "    return dataset_x, np.array(dataset_y)\n",
    "\n",
    "# To do sliding window on the data\n",
    "def sliding_window(data_X, data_Y, window_size, window_stride):\n",
    "    dataset_X_w_sliding = []\n",
    "    dataset_Y_w_sliding = []\n",
    "    for i in range(len(data_X)):\n",
    "        for j in range(0, len(data_X[i]) - window_size+1, window_stride):\n",
    "            dataset_X_w_sliding.append(data_X[i][j:j + window_size])\n",
    "            dataset_Y_w_sliding.append(data_Y[i])\n",
    "    return np.array(dataset_X_w_sliding), np.array(dataset_Y_w_sliding)\n",
    "\n",
    "\n",
    "# load dataset\n",
    "dataset_x, dataset_y = load_dataset(\"Dataset\")\n",
    "\n",
    "# split into train and test sets\n",
    "train_x, test_x, train_y, test_y = train_test_split(dataset_x, dataset_y, test_size=0.2, stratify = dataset_y, random_state=666)\n",
    "\n",
    "# combine the training data with idle data\n",
    "# dataset_x_idle, dataset_y_idle = load_idle_dataset(\"Dataset\")\n",
    "# train_x.extend(dataset_x_idle)\n",
    "# train_y = np.concatenate((train_y, dataset_y_idle), axis=0)\n",
    "\n",
    "# sliding window after train_test_split\n",
    "train_x, train_y = sliding_window(train_x, train_y, window_size, window_stride)\n",
    "test_x, test_y = sliding_window(test_x, test_y, window_size, window_stride)\n",
    "\n",
    "# print dataset size after sliding window\n",
    "print(\"Dataset size after sliding window: \" + str(len(train_y)))\n",
    "\n",
    "# calculate class weights\n",
    "class_weights = {}\n",
    "for i in range(len(action_types)):\n",
    "    class_weights[i] = 1 / dataset_y[:, i].sum()\n",
    "\n",
    "# print dataset disribution\n",
    "train_y_temp = np.argmax(train_y, axis=1)\n",
    "for i in range(len(action_types)):\n",
    "    print(\"Class \" + str(i) + \" has \" + str(train_y_temp.tolist().count(i)) + \" samples\")\n",
    "\n",
    "# backup test_x for c_sim and cosim\n",
    "test_x_copy = copy.deepcopy(test_x)\n",
    "\n",
    "# convert data from int16 to float32\n",
    "train_x, test_x = np.float32(train_x)/4096, np.float32(test_x)/4096\n",
    "# print(\"sample test x data: \" + str(test_x[0]))\n",
    "\n",
    "# summary of test dataset\n",
    "test = np.argmax(test_y, axis=1)\n",
    "print(\"\\nTest set distribution\")\n",
    "for i in range(len(action_types)):\n",
    "    print(\"Class \" + str(i) + \" has \" + str(test.tolist().count(i)) + \" samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1filters = 10\n",
    "conv1kernel = 15\n",
    "conv1stride = 5\n",
    "\n",
    "clear_session()\n",
    "model = Sequential()\n",
    "model.add(Conv1D(conv1filters, conv1kernel, strides=conv1stride, activation='relu', input_shape=(window_size, 6)))\n",
    "model.add(Dropout(0.5)) # 50% dropout\n",
    "# model.add(Conv1D(32, 10, activation='relu'))\n",
    "model.add(Flatten())\n",
    "# model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(len(action_types)))\n",
    "model.add(Softmax())\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# performance before training\n",
    "y_prediction = np.argmax(model.predict(test_x), axis=1)\n",
    "result = confusion_matrix(np.argmax(test_y, axis=1), y_prediction)\n",
    "sns.heatmap(result, annot=True, fmt=\"d\", xticklabels=action_types, yticklabels=action_types)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint callback\n",
    "checkpoint_filepath = \"model_checkpoint/\"\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "\n",
    "# learning rate reduce on plateau callback\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=50, min_lr=0)\n",
    "\n",
    "# early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=200, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Train the model\n",
    "def train_network(model, train_x, train_y, test_x, test_y):\n",
    "    verbose = 1 # 0 for no logging to stdout, 1 for progress bar logging, 2 for one log line per epoch.\n",
    "    epochs = 1000\n",
    "    batch_size = 64\n",
    "    model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, validation_data=(test_x, test_y), class_weight = class_weights, callbacks = [model_checkpoint_callback, reduce_lr, early_stopping], verbose=verbose)\n",
    "    _, accuracy = model.evaluate(test_x, test_y, batch_size=batch_size, verbose=0)\n",
    "    return model\n",
    "\n",
    "start_time = time.time()\n",
    "model = train_network(model, train_x, train_y, test_x, test_y)\n",
    "print(\"Training time: %s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# model.load_weights(checkpoint_filepath)\n",
    "y_prediction = model.predict(test_x)\n",
    "\n",
    "threshold = 0.7\n",
    "# remove all the prediction with probability less than threshold\n",
    "filtered_pred = []\n",
    "filtered_test = []\n",
    "for i in range(len(y_prediction)):\n",
    "    if np.max(y_prediction[i]) > threshold:\n",
    "        filtered_pred.append(np.argmax(y_prediction[i]))\n",
    "        filtered_test.append(np.argmax(test_y[i]))\n",
    "\n",
    "# print proportion of filtered prediction\n",
    "print(\"Proportion of filtered prediction: \" + str(len(filtered_pred)/len(y_prediction)))\n",
    "\n",
    "result = confusion_matrix(filtered_test, filtered_pred)\n",
    "\n",
    "# result = confusion_matrix(np.argmax(test_y, axis=1), y_prediction)\n",
    "sns.heatmap(result, annot=True, fmt=\"d\", xticklabels=action_types, yticklabels=action_types)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "# print accuracy\n",
    "print(\"Accuracy: \" + str(np.sum(np.diag(result))/np.sum(result)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"window shape: {window_size, data_depth}\")\n",
    "print(f\"kernel shape: {conv1kernel, data_depth}\")\n",
    "print(f\"first layer weights shape: {model.layers[0].get_weights()[0].shape}\")\n",
    "print(f\"first layer output shape: {model.layers[0].output_shape}\")\n",
    "print(f\"second layer weights shape: {model.layers[3].get_weights()[0].shape}\")\n",
    "print(f\"second layer output shape: {model.layers[3].output_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out weights and biases one by one\n",
    "layers_indexes = [0, 3]\n",
    "\n",
    "for layer_index in layers_indexes:\n",
    "    layer = model.layers[layer_index]\n",
    "    layer_name = layer.name\n",
    "    weights = layer.get_weights()\n",
    "    print(weights[0].shape)\n",
    "    print(f\"INPUT_DTYPE model_param_{layer_name}_weights\")\n",
    "    for i in range(weights[0].shape[-1]):\n",
    "        print(\"index\", i)\n",
    "        print(np.transpose(np.transpose(weights[0])[i]))\n",
    "    print(f\"INPUT_DTYPE model_param_{layer_name}_biases\")\n",
    "    print(weights[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directly print out weights and biases\n",
    "layers_indexes = [0, 3]\n",
    "\n",
    "for layer_index in layers_indexes:\n",
    "    layer = model.layers[layer_index]\n",
    "    layer_name = layer.name\n",
    "    weights = layer.get_weights()\n",
    "    if layer_index == 0:\n",
    "        layer_name = \"CNN\"\n",
    "        weights_size_definition = \"[CNN_KERNEL_LENGTH][CNN_KERNEL_DEPTH][CNN_KERNEL_COUNT]\"\n",
    "        bias_size_definition = \"[CNN_KERNEL_COUNT]\"\n",
    "    else:\n",
    "        layer_name = \"dense\"\n",
    "        weights_size_definition = \"[DENSE_INPUT_NODES][DENSE_OUTPUT_NODES]\"\n",
    "        bias_size_definition = \"[DENSE_OUTPUT_NODES]\"\n",
    "    print(f\"static INPUT_DTYPE {layer_name}_weights{weights_size_definition} = {{\" + \", \".join([str(x) for x in weights[0].reshape(-1)]) + \"};\")\n",
    "    print(f\"static INPUT_DTYPE {layer_name}_bias{bias_size_definition} = {{\" + \", \".join([str(x) for x in weights[1]]) + \"};\")\n",
    "\n",
    "    # save weights and biases to file\n",
    "    np.save(f\"{layer_name}_weights.npy\", weights[0])\n",
    "    np.save(f\"{layer_name}_bias.npy\", weights[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directly print out test dataset\n",
    "n_values_per_line = 10\n",
    "dataset_size = len(test_x)\n",
    "# dataset_size = 1\n",
    "dataset_start_index = 0\n",
    "print(\"#define DATASET_SIZE\", dataset_size)\n",
    "\n",
    "# print out float test dataset\n",
    "print(f\"const float test_x[DATASET_SIZE][INPUT_LENGTH][INPUT_DEPTH] = {{\")\n",
    "for text_x_index in range(dataset_start_index, min(test_x.shape[0], dataset_start_index + dataset_size)):\n",
    "    for datapoint_index in range(0, len(test_x[text_x_index])):\n",
    "        for i in range(0, len(test_x[text_x_index][datapoint_index]), n_values_per_line):\n",
    "            print(\", \".join([str(x) for x in test_x_copy[text_x_index][datapoint_index][i:i+n_values_per_line]]) + \",\")\n",
    "print(\"};\") \n",
    "    \n",
    "print(f\"const int test_y[DATASET_SIZE][DENSE_OUTPUT_NODES] = {{\")\n",
    "for text_x_index in range(dataset_start_index, min(test_x.shape[0], dataset_start_index + dataset_size)):\n",
    "    for i in range(0, len(test_y[text_x_index]), n_values_per_line):\n",
    "        print(\", \".join([str(int(x)) for x in test_y[text_x_index][i:i+n_values_per_line]]) + \",\")\n",
    "print(\"};\")\n",
    "\n",
    "# save the test dataset\n",
    "np.save(\"test_x.npy\", test_x_copy)\n",
    "np.save(\"test_y.npy\", test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print model output without the last layer\n",
    "from keras.models import Model\n",
    "model_without_last_layer = Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "model_without_last_layer.summary()\n",
    "predicted_result = model_without_last_layer.predict(test_x)[0]\n",
    "print(predicted_result.shape)\n",
    "print(predicted_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('capstone')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f82d4ec2f4e1a7949fc551b5039d8b80c5fc6c2b366144cfac1fa6211cdc80ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
