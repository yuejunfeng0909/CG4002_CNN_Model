{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process dataset\n",
    "Extract data from the txt file for each user. We only extract the data for the time frame where the user is doing one action, not across different actions. Then we segment the data into 2 seconds window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initialized with size: 3218\n",
      "Number of data for class 1: 1161\n",
      "Number of data for class 2: 1078\n",
      "Number of data for class 3: 979\n",
      "shape of data: (6, 100)\n"
     ]
    }
   ],
   "source": [
    "# from scipy import stats\n",
    "\n",
    "# def extract_features(data):\n",
    "#     fft = []\n",
    "#     for i in range(data.shape[0]):\n",
    "#         fft.extend(np.float32(np.abs(np.fft.fft(data[i]))[:window_size//2] / window_size))\n",
    "#     return np.float32(fft)\n",
    "\n",
    "class MotionDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, data_directory, window_size=100, is_train=True, transform=None):\n",
    "        self.data_directory = data_directory\n",
    "        self.window_size = window_size\n",
    "        self.is_train = is_train\n",
    "        self.dataset_x = []\n",
    "        self.dataset_y = []\n",
    "        self._initialize_dataset()\n",
    "\n",
    "    def _initialize_dataset(self):\n",
    "        raw_labels = open(self.data_directory + \"/labels.txt\", \"r\")\n",
    "        self.dataset_x = []\n",
    "        self.dataset_y = []\n",
    "        current_exp, current_user = 0, 0\n",
    "        acc_current_file_lines = []\n",
    "        gyro_current_file_lines = []\n",
    "        for line in raw_labels:\n",
    "            # get the experiment and user data\n",
    "            line_split = list(map(int,line.split()))\n",
    "\n",
    "            # filter out the transition movements\n",
    "            if line_split[2] >= 4:\n",
    "                continue\n",
    "\n",
    "            # open new file if the current experiment and user are different from the previous run\n",
    "            if line_split[0] != current_exp or line_split[1] != current_user:\n",
    "                current_exp, current_user = line_split[0], line_split[1]\n",
    "\n",
    "                # Get the accelerometer data\n",
    "                current_file = open(self.data_directory + \"/acc_exp\" + str.zfill(str(current_exp), 2) + \"_user\" + str.zfill(str(current_user), 2) + \".txt\", \"r\")\n",
    "                # print(\"Opened file: \" + current_file.name)\n",
    "                # process lines\n",
    "                acc_current_file_lines = []\n",
    "                for file_line in current_file:\n",
    "                    acc_current_file_lines.append(list(map(float, file_line.split())))\n",
    "                current_file.close()\n",
    "\n",
    "                # Get the gyro data\n",
    "                current_file = open(self.data_directory + \"/gyro_exp\" + str.zfill(str(current_exp), 2) + \"_user\" + str.zfill(str(current_user), 2) + \".txt\", \"r\")\n",
    "                # print(\"Opened file: \" + current_file.name)\n",
    "                gyro_current_file_lines = []\n",
    "                for file_line in current_file:\n",
    "                    gyro_current_file_lines.append(list(map(float, file_line.split())))\n",
    "                current_file.close()\n",
    "            \n",
    "            # get the label, start and end indices\n",
    "            label, start, end = line_split[2:5]\n",
    "            label -= 1 # convert to 0-indexed\n",
    "\n",
    "            # sliding window\n",
    "            for i in range(start, end - window_size, window_size):\n",
    "                # calculate fft for the window\n",
    "                acc_window = acc_current_file_lines[i:i+window_size]\n",
    "                gyro_window = gyro_current_file_lines[i:i+window_size]\n",
    "                inputs = np.concatenate((acc_window, gyro_window), axis=1)\n",
    "                inputs = np.float32(np.transpose(inputs))\n",
    "                self.dataset_x.append(inputs)\n",
    "                self.dataset_y.append(label)\n",
    "\n",
    "        raw_labels.close()\n",
    "\n",
    "        print(\"Dataset initialized with size: \" + str(len(self.dataset_y)))\n",
    "        print(\"Number of data for class 1: \" + str(self.dataset_y.count(0)))\n",
    "        print(\"Number of data for class 2: \" + str(self.dataset_y.count(1)))\n",
    "        print(\"Number of data for class 3: \" + str(self.dataset_y.count(2)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset_x[idx], self.dataset_y[idx]\n",
    "\n",
    "window_size = 100   # 50Hz, 100 samples = 2s of movement\n",
    "dataset = MotionDataset(\"HAPT Data Set/RawData\", window_size)\n",
    "print(\"shape of data: \" + str(dataset[0][0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 100)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample data\n",
    "dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training and testing data\n",
    "batch_size = 32\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv1d(6, 64, kernel_size=(6,), stride=(1,))\n",
       "  (1): ReLU()\n",
       "  (2): Conv1d(64, 32, kernel_size=(6,), stride=(1,))\n",
       "  (3): ReLU()\n",
       "  (4): Flatten(start_dim=1, end_dim=-1)\n",
       "  (5): LazyLinear(in_features=0, out_features=3, bias=True)\n",
       "  (6): Softmax(dim=None)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = dataset[0][0].shape\n",
    "print(input_size)\n",
    "\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Conv1d(6, 64, kernel_size=6, stride=1, padding=0),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv1d(64, 32, kernel_size=6, stride=1, padding=0),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.LazyLinear(3),\n",
    "    torch.nn.Softmax()\n",
    ")\n",
    "\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001, amsgrad=True)\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_valid_loss = np.inf\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch: 1] \t Training Loss: 0.858379 \t Validation Loss: 0.775911 \t Accuracy: 0.770186\n",
      "evaluation loss reduced, model saved\n",
      "[Epoch: 2] \t Training Loss: 0.729318 \t Validation Loss: 0.694526 \t Accuracy: 0.850932\n",
      "evaluation loss reduced, model saved\n",
      "[Epoch: 3] \t Training Loss: 0.672306 \t Validation Loss: 0.638002 \t Accuracy: 0.923913\n",
      "evaluation loss reduced, model saved\n",
      "[Epoch: 4] \t Training Loss: 0.625124 \t Validation Loss: 0.613664 \t Accuracy: 0.944099\n",
      "evaluation loss reduced, model saved\n",
      "[Epoch: 5] \t Training Loss: 0.602788 \t Validation Loss: 0.601747 \t Accuracy: 0.947205\n",
      "evaluation loss reduced, model saved\n",
      "[Epoch: 6] \t Training Loss: 0.585867 \t Validation Loss: 0.581793 \t Accuracy: 0.978261\n",
      "evaluation loss reduced, model saved\n",
      "[Epoch: 7] \t Training Loss: 0.582256 \t Validation Loss: 0.587110 \t Accuracy: 0.967391\n",
      "[Epoch: 8] \t Training Loss: 0.571631 \t Validation Loss: 0.573777 \t Accuracy: 0.976708\n",
      "evaluation loss reduced, model saved\n",
      "[Epoch: 9] \t Training Loss: 0.570180 \t Validation Loss: 0.575631 \t Accuracy: 0.976708\n",
      "[Epoch:10] \t Training Loss: 0.560345 \t Validation Loss: 0.562706 \t Accuracy: 0.990683\n",
      "evaluation loss reduced, model saved\n",
      "[Epoch:11] \t Training Loss: 0.557914 \t Validation Loss: 0.562320 \t Accuracy: 0.995342\n",
      "evaluation loss reduced, model saved\n",
      "[Epoch:12] \t Training Loss: 0.557590 \t Validation Loss: 0.572130 \t Accuracy: 0.982919\n",
      "[Epoch:13] \t Training Loss: 0.558877 \t Validation Loss: 0.569163 \t Accuracy: 0.992236\n",
      "[Epoch:14] \t Training Loss: 0.554921 \t Validation Loss: 0.559633 \t Accuracy: 0.993789\n",
      "evaluation loss reduced, model saved\n",
      "[Epoch:15] \t Training Loss: 0.553926 \t Validation Loss: 0.559257 \t Accuracy: 0.995342\n",
      "evaluation loss reduced, model saved\n",
      "[Epoch:16] \t Training Loss: 0.553220 \t Validation Loss: 0.560829 \t Accuracy: 0.990683\n",
      "[Epoch:17] \t Training Loss: 0.552794 \t Validation Loss: 0.557648 \t Accuracy: 0.996894\n",
      "evaluation loss reduced, model saved\n",
      "[Epoch:18] \t Training Loss: 0.552708 \t Validation Loss: 0.556685 \t Accuracy: 0.998447\n",
      "evaluation loss reduced, model saved\n",
      "[Epoch:19] \t Training Loss: 0.552623 \t Validation Loss: 0.559497 \t Accuracy: 0.993789\n",
      "[Epoch:20] \t Training Loss: 0.552583 \t Validation Loss: 0.557473 \t Accuracy: 0.996894\n",
      "[Epoch:21] \t Training Loss: 0.552577 \t Validation Loss: 0.557261 \t Accuracy: 0.995342\n",
      "[Epoch:22] \t Training Loss: 0.552928 \t Validation Loss: 0.569813 \t Accuracy: 0.982919\n",
      "[Epoch:23] \t Training Loss: 0.563612 \t Validation Loss: 0.556493 \t Accuracy: 0.995342\n",
      "evaluation loss reduced, model saved\n",
      "[Epoch:24] \t Training Loss: 0.554245 \t Validation Loss: 0.558298 \t Accuracy: 0.995342\n",
      "Epoch 00024: reducing learning rate of group 0 to 5.0000e-04.\n",
      "[Epoch:25] \t Training Loss: 0.552797 \t Validation Loss: 0.557391 \t Accuracy: 0.998447\n",
      "[Epoch:26] \t Training Loss: 0.552668 \t Validation Loss: 0.555494 \t Accuracy: 0.998447\n",
      "evaluation loss reduced, model saved\n",
      "[Epoch:27] \t Training Loss: 0.552158 \t Validation Loss: 0.558771 \t Accuracy: 0.998447\n",
      "[Epoch:28] \t Training Loss: 0.552101 \t Validation Loss: 0.555631 \t Accuracy: 0.998447\n",
      "[Epoch:29] \t Training Loss: 0.552068 \t Validation Loss: 0.554931 \t Accuracy: 0.998447\n",
      "evaluation loss reduced, model saved\n",
      "[Epoch:30] \t Training Loss: 0.552044 \t Validation Loss: 0.556250 \t Accuracy: 0.996894\n",
      "Epoch 00030: reducing learning rate of group 0 to 2.5000e-04.\n",
      "[Epoch:31] \t Training Loss: 0.552031 \t Validation Loss: 0.555441 \t Accuracy: 0.998447\n",
      "[Epoch:32] \t Training Loss: 0.552025 \t Validation Loss: 0.555316 \t Accuracy: 1.000000\n",
      "[Epoch:33] \t Training Loss: 0.552021 \t Validation Loss: 0.555274 \t Accuracy: 1.000000\n",
      "[Epoch:34] \t Training Loss: 0.552013 \t Validation Loss: 0.555349 \t Accuracy: 1.000000\n",
      "[Epoch:35] \t Training Loss: 0.552005 \t Validation Loss: 0.555593 \t Accuracy: 0.996894\n",
      "[Epoch:36] \t Training Loss: 0.552001 \t Validation Loss: 0.559089 \t Accuracy: 1.000000\n",
      "[Epoch:37] \t Training Loss: 0.551998 \t Validation Loss: 0.555423 \t Accuracy: 0.996894\n",
      "[Epoch:38] \t Training Loss: 0.551990 \t Validation Loss: 0.555413 \t Accuracy: 0.996894\n",
      "Epoch 00038: reducing learning rate of group 0 to 1.2500e-04.\n",
      "[Epoch:39] \t Training Loss: 0.551984 \t Validation Loss: 0.555301 \t Accuracy: 0.998447\n",
      "[Epoch:40] \t Training Loss: 0.551983 \t Validation Loss: 0.555281 \t Accuracy: 0.998447\n",
      "[Epoch:41] \t Training Loss: 0.551982 \t Validation Loss: 0.555217 \t Accuracy: 0.998447\n",
      "[Epoch:42] \t Training Loss: 0.551980 \t Validation Loss: 0.555280 \t Accuracy: 0.998447\n",
      "[Epoch:43] \t Training Loss: 0.551978 \t Validation Loss: 0.555269 \t Accuracy: 0.998447\n",
      "[Epoch:44] \t Training Loss: 0.551976 \t Validation Loss: 0.556184 \t Accuracy: 0.998447\n",
      "Epoch 00044: reducing learning rate of group 0 to 6.2500e-05.\n",
      "[Epoch:45] \t Training Loss: 0.551974 \t Validation Loss: 0.555146 \t Accuracy: 0.998447\n",
      "[Epoch:46] \t Training Loss: 0.551974 \t Validation Loss: 0.555172 \t Accuracy: 0.998447\n",
      "[Epoch:47] \t Training Loss: 0.551972 \t Validation Loss: 0.555219 \t Accuracy: 0.998447\n",
      "[Epoch:48] \t Training Loss: 0.551972 \t Validation Loss: 0.555171 \t Accuracy: 0.998447\n",
      "[Epoch:49] \t Training Loss: 0.551970 \t Validation Loss: 0.555337 \t Accuracy: 0.998447\n",
      "[Epoch:50] \t Training Loss: 0.551970 \t Validation Loss: 0.555200 \t Accuracy: 0.998447\n",
      "Epoch 00050: reducing learning rate of group 0 to 3.1250e-05.\n",
      "[Epoch:51] \t Training Loss: 0.551968 \t Validation Loss: 0.555194 \t Accuracy: 0.998447\n",
      "[Epoch:52] \t Training Loss: 0.551969 \t Validation Loss: 0.555195 \t Accuracy: 0.998447\n",
      "[Epoch:53] \t Training Loss: 0.551969 \t Validation Loss: 0.555191 \t Accuracy: 0.998447\n",
      "[Epoch:54] \t Training Loss: 0.551969 \t Validation Loss: 0.555177 \t Accuracy: 0.998447\n",
      "[Epoch:55] \t Training Loss: 0.551967 \t Validation Loss: 0.555182 \t Accuracy: 0.998447\n",
      "[Epoch:56] \t Training Loss: 0.551966 \t Validation Loss: 0.557796 \t Accuracy: 0.998447\n",
      "Epoch 00056: reducing learning rate of group 0 to 1.5625e-05.\n",
      "[Epoch:57] \t Training Loss: 0.551969 \t Validation Loss: 0.555160 \t Accuracy: 0.998447\n",
      "[Epoch:58] \t Training Loss: 0.551966 \t Validation Loss: 0.555164 \t Accuracy: 0.998447\n",
      "[Epoch:59] \t Training Loss: 0.551966 \t Validation Loss: 0.555184 \t Accuracy: 0.998447\n",
      "[Epoch:60] \t Training Loss: 0.551966 \t Validation Loss: 0.555166 \t Accuracy: 0.998447\n",
      "[Epoch:61] \t Training Loss: 0.551965 \t Validation Loss: 0.555167 \t Accuracy: 0.998447\n",
      "[Epoch:62] \t Training Loss: 0.551965 \t Validation Loss: 0.555169 \t Accuracy: 0.998447\n",
      "Epoch 00062: reducing learning rate of group 0 to 7.8125e-06.\n",
      "[Epoch:63] \t Training Loss: 0.551966 \t Validation Loss: 0.555163 \t Accuracy: 0.998447\n",
      "[Epoch:64] \t Training Loss: 0.551965 \t Validation Loss: 0.555162 \t Accuracy: 0.998447\n",
      "[Epoch:65] \t Training Loss: 0.551965 \t Validation Loss: 0.555160 \t Accuracy: 0.998447\n",
      "[Epoch:66] \t Training Loss: 0.551965 \t Validation Loss: 0.555163 \t Accuracy: 0.998447\n",
      "[Epoch:67] \t Training Loss: 0.551965 \t Validation Loss: 0.555163 \t Accuracy: 0.998447\n",
      "[Epoch:68] \t Training Loss: 0.551964 \t Validation Loss: 0.555194 \t Accuracy: 0.998447\n",
      "Epoch 00068: reducing learning rate of group 0 to 3.9063e-06.\n",
      "[Epoch:69] \t Training Loss: 0.551964 \t Validation Loss: 0.555164 \t Accuracy: 0.998447\n",
      "[Epoch:70] \t Training Loss: 0.551964 \t Validation Loss: 0.558685 \t Accuracy: 0.998447\n",
      "[Epoch:71] \t Training Loss: 0.551964 \t Validation Loss: 0.559032 \t Accuracy: 0.998447\n",
      "[Epoch:72] \t Training Loss: 0.551964 \t Validation Loss: 0.555156 \t Accuracy: 0.998447\n",
      "[Epoch:73] \t Training Loss: 0.551964 \t Validation Loss: 0.555183 \t Accuracy: 0.998447\n",
      "[Epoch:74] \t Training Loss: 0.551964 \t Validation Loss: 0.555157 \t Accuracy: 0.998447\n",
      "Epoch 00074: reducing learning rate of group 0 to 1.9531e-06.\n",
      "[Epoch:75] \t Training Loss: 0.551967 \t Validation Loss: 0.555156 \t Accuracy: 0.998447\n",
      "[Epoch:76] \t Training Loss: 0.551966 \t Validation Loss: 0.555164 \t Accuracy: 0.998447\n",
      "[Epoch:77] \t Training Loss: 0.551964 \t Validation Loss: 0.555159 \t Accuracy: 0.998447\n",
      "[Epoch:78] \t Training Loss: 0.551964 \t Validation Loss: 0.555462 \t Accuracy: 0.998447\n",
      "[Epoch:79] \t Training Loss: 0.551965 \t Validation Loss: 0.555157 \t Accuracy: 0.998447\n",
      "[Epoch:80] \t Training Loss: 0.552459 \t Validation Loss: 0.555157 \t Accuracy: 0.998447\n",
      "Epoch 00080: reducing learning rate of group 0 to 9.7656e-07.\n",
      "[Epoch:81] \t Training Loss: 0.551964 \t Validation Loss: 0.555158 \t Accuracy: 0.998447\n",
      "[Epoch:82] \t Training Loss: 0.551964 \t Validation Loss: 0.555159 \t Accuracy: 0.998447\n",
      "[Epoch:83] \t Training Loss: 0.551964 \t Validation Loss: 0.555159 \t Accuracy: 0.998447\n",
      "[Epoch:84] \t Training Loss: 0.551964 \t Validation Loss: 0.555157 \t Accuracy: 0.998447\n",
      "[Epoch:85] \t Training Loss: 0.551968 \t Validation Loss: 0.556577 \t Accuracy: 0.998447\n",
      "[Epoch:86] \t Training Loss: 0.551964 \t Validation Loss: 0.558764 \t Accuracy: 0.998447\n",
      "Epoch 00086: reducing learning rate of group 0 to 4.8828e-07.\n",
      "[Epoch:87] \t Training Loss: 0.551964 \t Validation Loss: 0.555343 \t Accuracy: 0.998447\n",
      "[Epoch:88] \t Training Loss: 0.551964 \t Validation Loss: 0.557206 \t Accuracy: 0.998447\n",
      "[Epoch:89] \t Training Loss: 0.551964 \t Validation Loss: 0.555186 \t Accuracy: 0.998447\n",
      "[Epoch:90] \t Training Loss: 0.551964 \t Validation Loss: 0.555184 \t Accuracy: 0.998447\n",
      "[Epoch:91] \t Training Loss: 0.551964 \t Validation Loss: 0.555158 \t Accuracy: 0.998447\n",
      "[Epoch:92] \t Training Loss: 0.551964 \t Validation Loss: 0.555158 \t Accuracy: 0.998447\n",
      "Epoch 00092: reducing learning rate of group 0 to 2.4414e-07.\n",
      "[Epoch:93] \t Training Loss: 0.551964 \t Validation Loss: 0.555158 \t Accuracy: 0.998447\n",
      "[Epoch:94] \t Training Loss: 0.551964 \t Validation Loss: 0.555158 \t Accuracy: 0.998447\n",
      "[Epoch:95] \t Training Loss: 0.551964 \t Validation Loss: 0.555852 \t Accuracy: 0.998447\n",
      "[Epoch:96] \t Training Loss: 0.551964 \t Validation Loss: 0.555158 \t Accuracy: 0.998447\n",
      "[Epoch:97] \t Training Loss: 0.551964 \t Validation Loss: 0.555158 \t Accuracy: 0.998447\n",
      "[Epoch:98] \t Training Loss: 0.551964 \t Validation Loss: 0.555158 \t Accuracy: 0.998447\n",
      "Epoch 00098: reducing learning rate of group 0 to 1.2207e-07.\n",
      "[Epoch:99] \t Training Loss: 0.551965 \t Validation Loss: 0.555174 \t Accuracy: 0.998447\n",
      "[Epoch:100] \t Training Loss: 0.551965 \t Validation Loss: 0.555159 \t Accuracy: 0.998447\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "net.train()\n",
    "def get_num_correct(preds, labels):\n",
    "    return preds.round().eq(labels).sum().item()\n",
    "\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    total_training_loss = 0.0\n",
    "    batch_count = 0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        batch_count += 1\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        total_training_loss += loss.item()\n",
    "        vaccuracy = 0\n",
    "            \n",
    "    valid_loss = 0.0\n",
    "    net.eval()     # Optional when not using Model Specific layer\n",
    "    vcount = 0\n",
    "    vcorrect = 0\n",
    "    for j, vdata in enumerate(test_loader, 0):\n",
    "        vcount += 1\n",
    "        vdata, vlabels = vdata[0].to(device), vdata[1].to(device)\n",
    "        # Forward Pass\n",
    "        target = net(vdata)\n",
    "        # Find the Loss\n",
    "        loss = criterion(target,vlabels)\n",
    "        # Calculate Loss\n",
    "        valid_loss += loss.item()\n",
    "        # Calculate accuracy\n",
    "        vcorrect += (target.argmax(1) == vlabels).sum().item()\n",
    "    valid_loss = valid_loss / vcount\n",
    "    vaccuracy = vcorrect / len(test_loader.dataset)\n",
    "    net.train()\n",
    "    print(f'[Epoch:{epoch + 1:2d}] \\t Training Loss: {running_loss / batch_count:5f} \\t Validation Loss: {valid_loss:5f} \\t Accuracy: {vaccuracy:5f}')\n",
    "    \n",
    "    # save model if loss improved, and obtain predictions\n",
    "    if (valid_loss < min_valid_loss):\n",
    "        min_valid_loss = valid_loss\n",
    "        torch.save(net, \"model.pt\")\n",
    "        print(\"evaluation loss reduced, model saved\")\n",
    "    scheduler.step(vaccuracy)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('capstone')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f82d4ec2f4e1a7949fc551b5039d8b80c5fc6c2b366144cfac1fa6211cdc80ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
