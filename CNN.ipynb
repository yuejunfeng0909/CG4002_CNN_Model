{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from keras.backend import clear_session\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(2)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process dataset\n",
    "Extract data from the txt file for each user. We only extract the data for the time frame where the user is doing one action, not across different actions. Then we segment the data into 2 seconds window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file loader\n",
    "from os.path import isfile\n",
    "\n",
    "def load_data(file):\n",
    "    data = pd.read_csv(file, header=None)\n",
    "    return data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 75   # 50Hz, 75 samples = 1.5s of movement\n",
    "window_stride = 25\n",
    "action_types = ('shield', 'reload', 'grenade', 'final')\n",
    "data_depth = 6\n",
    "\n",
    "dataset_users = [0, 1, 2, 3, 4]\n",
    "\n",
    "# Function to load dataset\n",
    "def load_dataset(data_dir):\n",
    "    dataset_x = []\n",
    "    dataset_y = []\n",
    "    \n",
    "    for i in dataset_users:\n",
    "        user_data_dir = f\"{data_dir}/user{i}\"\n",
    "        print(f\"Loading data from {user_data_dir}\")\n",
    "    \n",
    "        for action_type in action_types:\n",
    "\n",
    "            for i in range(1, 100, 1):\n",
    "                # if file exists\n",
    "                if not isfile(user_data_dir + f\"/{action_type}{i}.csv\"):\n",
    "                    # print number of files loaded\n",
    "                    print(f\"Loaded {i-1} {action_type} files\")\n",
    "                    break\n",
    "                data = load_data(user_data_dir + f\"/{action_type}{i}.csv\")\n",
    "                dataset_x.append(np.int32(data))\n",
    "                dataset_y.append(action_types.index(action_type))\n",
    "\n",
    "    print(\"Dataset initialized with size: \" + str(len(dataset_y)))\n",
    "    for i in range(len(action_types)):\n",
    "        print(\"Class \" + str(i) + \" has \" + str(dataset_y.count(i)) + \" samples\")\n",
    "    dataset_y = to_categorical(dataset_y)\n",
    "    return dataset_x, np.array(dataset_y)\n",
    "\n",
    "def load_idle_dataset(data_dir):\n",
    "    dataset_x = []\n",
    "    dataset_y = []\n",
    "    \n",
    "    for i in dataset_users:\n",
    "        user_data_dir = f\"{data_dir}/user{i}\"\n",
    "        print(f\"Loading idle data from {user_data_dir}\")\n",
    "    \n",
    "        for i in range(1, 100, 1):\n",
    "            # if file exists\n",
    "            if not isfile(user_data_dir + f\"/idle{i}.csv\"):\n",
    "                break\n",
    "            data = load_data(user_data_dir + f\"/idle{i}.csv\")\n",
    "            dataset_x.append(data)\n",
    "            dataset_y.append([0.25, 0.25, 0.25, 0.25])\n",
    "\n",
    "    print(\"Idle dataset initialized with size: \" + str(len(dataset_y)))\n",
    "    return dataset_x, np.array(dataset_y)\n",
    "\n",
    "# To do sliding window on the data\n",
    "def sliding_window(data_X, data_Y, window_size, window_stride):\n",
    "    dataset_X_w_sliding = []\n",
    "    dataset_Y_w_sliding = []\n",
    "    for i in range(len(data_X)):\n",
    "        for j in range(0, len(data_X[i]) - window_size+1, window_stride):\n",
    "            dataset_X_w_sliding.append(data_X[i][j:j + window_size])\n",
    "            dataset_Y_w_sliding.append(data_Y[i])\n",
    "    return np.array(dataset_X_w_sliding), np.array(dataset_Y_w_sliding)\n",
    "\n",
    "\n",
    "# load dataset\n",
    "dataset_x, dataset_y = load_dataset(\"Dataset\")\n",
    "\n",
    "# split into train and test sets\n",
    "train_x, test_x, train_y, test_y = train_test_split(dataset_x, dataset_y, test_size=0.2, stratify = dataset_y, random_state=66)\n",
    "\n",
    "# backup test set for evaluation\n",
    "test_x_eval = copy.deepcopy(test_x)\n",
    "test_y_eval = copy.deepcopy(test_y)\n",
    "\n",
    "# combine the training data with idle data\n",
    "dataset_x_idle, dataset_y_idle = load_idle_dataset(\"Dataset\")\n",
    "train_x.extend(dataset_x_idle)\n",
    "train_y = np.concatenate((train_y, dataset_y_idle), axis=0)\n",
    "\n",
    "# sliding window after train_test_split\n",
    "train_x, train_y = sliding_window(train_x, train_y, window_size, window_stride)\n",
    "test_x, test_y = sliding_window(test_x, test_y, window_size, window_stride)\n",
    "\n",
    "# print dataset size after sliding window\n",
    "print(\"Dataset size after sliding window: \" + str(len(train_y)))\n",
    "\n",
    "# calculate class weights\n",
    "class_weights = {}\n",
    "for i in range(len(action_types)):\n",
    "    class_weights[i] = 1 / dataset_y[:, i].sum()\n",
    "\n",
    "# print dataset disribution\n",
    "train_y_temp = np.argmax(train_y, axis=1)\n",
    "for i in range(len(action_types)):\n",
    "    print(\"Class \" + str(i) + \" has \" + str(train_y_temp.tolist().count(i)) + \" samples\")\n",
    "\n",
    "# backup test_x for c_sim and cosim\n",
    "test_x_copy = copy.deepcopy(test_x)\n",
    "\n",
    "# convert data from int16 to float32\n",
    "train_x, test_x = np.float32(train_x)/4096, np.float32(test_x)/4096\n",
    "# print(\"sample test x data: \" + str(test_x[0]))\n",
    "\n",
    "# summary of test dataset\n",
    "test = np.argmax(test_y, axis=1)\n",
    "print(\"\\nTest set distribution\")\n",
    "for i in range(len(action_types)):\n",
    "    print(\"Class \" + str(i) + \" has \" + str(test.tolist().count(i)) + \" samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation methods\n",
    "\n",
    "from scipy.interpolate import CubicSpline      # for warping\n",
    "from transforms3d.axangles import axangle2mat  # for rotation\n",
    "from tensorflow import keras\n",
    "\n",
    "# \n",
    "def DA_Jitter(X, sigma=0.05):\n",
    "    myNoise = np.random.normal(loc=0, scale=sigma, size=X.shape)\n",
    "    return X+myNoise\n",
    "\n",
    "# scaling\n",
    "def DA_Scaling(X, sigma=0.1):\n",
    "    scalingFactor = np.random.normal(loc=1.0, scale=sigma, size=(1,X.shape[1])) # shape=(1,3)\n",
    "    myNoise = np.matmul(np.ones((X.shape[0],1)), scalingFactor)\n",
    "    print(myNoise.shape)\n",
    "    return X*myNoise\n",
    "\n",
    "# magnitude warping\n",
    "def GenerateRandomCurves(X, sigma=0.2, knot=4):\n",
    "    xx = (np.ones((X.shape[1],1))*(np.arange(0,X.shape[0], (X.shape[0]-1)/(knot+1)))).transpose()\n",
    "    yy = np.random.normal(loc=1.0, scale=sigma, size=(knot+2, X.shape[1]))\n",
    "    x_range = np.arange(X.shape[0])\n",
    "    cs_x = CubicSpline(xx[:,0], yy[:,0])\n",
    "    cs_y = CubicSpline(xx[:,1], yy[:,1])\n",
    "    cs_z = CubicSpline(xx[:,2], yy[:,2])\n",
    "    return np.array([cs_x(x_range),cs_y(x_range),cs_z(x_range)]).transpose()\n",
    "\n",
    "# time dostortion\n",
    "def DistortTimesteps(X, sigma=0.2):\n",
    "    tt = GenerateRandomCurves(X, sigma) # Regard these samples aroun 1 as time intervals\n",
    "    tt_cum = np.cumsum(tt, axis=0)        # Add intervals to make a cumulative graph\n",
    "    # Make the last value to have X.shape[0]\n",
    "    t_scale = [(X.shape[0]-1)/tt_cum[-1,0],(X.shape[0]-1)/tt_cum[-1,1],(X.shape[0]-1)/tt_cum[-1,2]]\n",
    "    tt_cum[:,0] = tt_cum[:,0]*t_scale[0]\n",
    "    tt_cum[:,1] = tt_cum[:,1]*t_scale[1]\n",
    "    tt_cum[:,2] = tt_cum[:,2]*t_scale[2]\n",
    "    return tt_cum\n",
    "\n",
    "# time warping\n",
    "def DA_TimeWarp(X, sigma=0.2):\n",
    "    tt_new = DistortTimesteps(X, sigma)\n",
    "    X_new = np.zeros(X.shape)\n",
    "    x_range = np.arange(X.shape[0])\n",
    "    X_new[:,0] = np.interp(x_range, tt_new[:,0], X[:,0])\n",
    "    X_new[:,1] = np.interp(x_range, tt_new[:,1], X[:,1])\n",
    "    X_new[:,2] = np.interp(x_range, tt_new[:,2], X[:,2])\n",
    "    return X_new\n",
    "\n",
    "# rotation\n",
    "def DA_Rotation(X):\n",
    "    axis = np.random.uniform(low=-1, high=1, size=3)\n",
    "    angle = np.random.uniform(low=-np.pi, high=np.pi)\n",
    "    accel = np.matmul(X[:, :, :3] , axangle2mat(axis,angle))\n",
    "    axis = np.random.uniform(low=-1, high=1, size=3)\n",
    "    angle = np.random.uniform(low=-np.pi, high=np.pi)\n",
    "    gyro = np.matmul(X[:, :, 3:] , axangle2mat(axis,angle))\n",
    "    return np.concatenate((accel, gyro), axis=2)\n",
    "\n",
    "# permutation ???\n",
    "def DA_Permutation(X, nPerm=4, minSegLength=10):\n",
    "    X_new = np.zeros(X.shape)\n",
    "    idx = np.random.permutation(nPerm)\n",
    "    bWhile = True\n",
    "    while bWhile == True:\n",
    "        segs = np.zeros(nPerm+1, dtype=int)\n",
    "        segs[1:-1] = np.sort(np.random.randint(minSegLength, X.shape[0]-minSegLength, nPerm-1))\n",
    "        segs[-1] = X.shape[0]\n",
    "        if np.min(segs[1:]-segs[0:-1]) > minSegLength:\n",
    "            bWhile = False\n",
    "    pp = 0\n",
    "    for ii in range(nPerm):\n",
    "        x_temp = X[segs[idx[ii]]:segs[idx[ii]+1],:]\n",
    "        X_new[pp:pp+len(x_temp),:] = x_temp\n",
    "        pp += len(x_temp)\n",
    "    return(X_new)\n",
    "\n",
    "class DataAugmentationCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, original_input: np.ndarray, augmented_input: np.ndarray, jitter_sigma=0.05, scaling_sigma=0.1, timewarp_sigma=0.2) -> None:\n",
    "        super().__init__()\n",
    "        self.original_input = original_input\n",
    "        self.augmented_input = augmented_input\n",
    "        self.jitter_sigma = jitter_sigma\n",
    "        self.scaling_sigma = scaling_sigma\n",
    "        self.timewarp_sigma = timewarp_sigma\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        # print sample data\n",
    "        X_new = DA_Jitter(self.original_input, self.jitter_sigma)\n",
    "        # X_new = DA_Scaling(X_new, self.scaling_sigma)\n",
    "        # X_new = DA_TimeWarp(X_new, self.timewarp_sigma)\n",
    "        X_new = DA_Rotation(X_new)\n",
    "        # X_new = DA_Permutation(X_new)\n",
    "        self.augmented_input[:] = X_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Flatten, Dropout, Conv1D, MaxPooling1D, Softmax, GlobalAveragePooling1D, InputLayer\n",
    "\n",
    "clear_session()\n",
    "model = Sequential()\n",
    "\n",
    "conv1filters = 32\n",
    "conv1kernel = 3\n",
    "conv1stride = 1\n",
    "model.add(InputLayer(input_shape=(window_size, data_depth)))\n",
    "model.add(Conv1D(conv1filters, conv1kernel, strides=conv1stride, activation='relu'))\n",
    "# model.add(Dropout(0.5)) # 50% dropout\n",
    "\n",
    "# conv2filters = 32\n",
    "# conv2kernel = 3\n",
    "# conv2stride = 1\n",
    "# model.add(Conv1D(conv2filters, conv2kernel, strides=conv2stride, activation='relu'))\n",
    "# model.add(Dropout(0.5)) # 50% dropout\n",
    "\n",
    "model.add(GlobalAveragePooling1D())\n",
    "# model.add(Flatten())\n",
    "\n",
    "# model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(len(action_types)))\n",
    "model.add(Softmax())\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# performance before training\n",
    "y_prediction = np.argmax(model.predict(test_x), axis=1)\n",
    "result = confusion_matrix(np.argmax(test_y, axis=1), y_prediction)\n",
    "sns.heatmap(result, annot=True, fmt=\"d\", xticklabels=action_types, yticklabels=action_types)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint callback\n",
    "checkpoint_filepath = \"model_checkpoint/\"\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "\n",
    "# learning rate reduce on plateau callback\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=50, min_lr=0)\n",
    "\n",
    "# early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=200, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Train the model\n",
    "def train_network(model, train_x, train_y, test_x, test_y):\n",
    "    verbose = 1 # 0 for no logging to stdout, 1 for progress bar logging, 2 for one log line per epoch.\n",
    "    epochs = 1000\n",
    "    batch_size = 32\n",
    "    train_x_augmented = np.zeros(train_x.shape)\n",
    "    history = model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, validation_data=(test_x, test_y), class_weight = class_weights, callbacks = [model_checkpoint_callback, reduce_lr, early_stopping, DataAugmentationCallback(train_x, train_x_augmented)], verbose=verbose)\n",
    "    # _, accuracy = model.evaluate(test_x, test_y, batch_size=batch_size, verbose=0)\n",
    "\n",
    "    return model, history\n",
    "\n",
    "start_time = time.time()\n",
    "model, history = train_network(model, train_x, train_y, test_x, test_y)\n",
    "print(\"Training time: %s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print validation loss history\n",
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "highest_false_confidence = 0\n",
    "\n",
    "model.load_weights(checkpoint_filepath)\n",
    "y_prediction = model.predict(test_x)\n",
    "\n",
    "# threshold = 0\n",
    "threshold = 0.8\n",
    "# remove all the prediction with probability less than threshold\n",
    "filtered_pred = []\n",
    "filtered_test = []\n",
    "for i in range(len(y_prediction)):\n",
    "    if np.max(y_prediction[i]) > threshold:\n",
    "        filtered_pred.append(np.argmax(y_prediction[i]))\n",
    "        filtered_test.append(np.argmax(test_y[i]))\n",
    "    if np.max(y_prediction[i]) > highest_false_confidence and np.argmax(y_prediction[i]) != np.argmax(test_y[i]):\n",
    "        highest_false_confidence = np.max(y_prediction[i])\n",
    "\n",
    "# print proportion of filtered prediction\n",
    "print(\"Proportion of filtered prediction: \" + str(len(filtered_pred)/len(y_prediction)))\n",
    "\n",
    "# print highest false confidence\n",
    "print(\"Highest false confidence: \" + str(highest_false_confidence))\n",
    "\n",
    "result = confusion_matrix(filtered_test, filtered_pred)\n",
    "\n",
    "# result = confusion_matrix(np.argmax(test_y, axis=1), y_prediction)\n",
    "sns.heatmap(result, annot=True, fmt=\"d\", xticklabels=action_types, yticklabels=action_types)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "# print accuracy\n",
    "print(\"Accuracy: \" + str(np.sum(np.diag(result))/np.sum(result)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "highest_false_confidence = 0\n",
    "\n",
    "model.load_weights(checkpoint_filepath)\n",
    "y_prediction = model.predict(test_x)\n",
    "\n",
    "# threshold = 0\n",
    "# threshold = 0.7\n",
    "# remove all the prediction with probability less than threshold\n",
    "filtered_pred = []\n",
    "filtered_test = []\n",
    "for i in range(len(y_prediction)):\n",
    "    if np.max(y_prediction[i]) > threshold:\n",
    "        filtered_pred.append(np.argmax(y_prediction[i]))\n",
    "        filtered_test.append(np.argmax(test_y[i]))\n",
    "    if np.max(y_prediction[i]) > highest_false_confidence and np.argmax(y_prediction[i]) != np.argmax(test_y[i]):\n",
    "        highest_false_confidence = np.max(y_prediction[i])\n",
    "\n",
    "# print proportion of filtered prediction\n",
    "print(\"Proportion of filtered prediction: \" + str(len(filtered_pred)/len(y_prediction)))\n",
    "\n",
    "# print highest false confidence\n",
    "print(\"Highest false confidence: \" + str(highest_false_confidence))\n",
    "\n",
    "result = confusion_matrix(filtered_test, filtered_pred)\n",
    "\n",
    "# result = confusion_matrix(np.argmax(test_y, axis=1), y_prediction)\n",
    "sns.heatmap(result, annot=True, fmt=\"d\", xticklabels=action_types, yticklabels=action_types)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "# print accuracy\n",
    "print(\"Accuracy: \" + str(np.sum(np.diag(result))/np.sum(result)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate actual performance\n",
    "pred = []\n",
    "actual = []\n",
    "highest_false_confidence = 0\n",
    "for i in range(len(test_x_eval)):\n",
    "    data = np.array(test_x_eval[i])\n",
    "    # sliding window until the prediction has probability greater than threshold, or end of the sequence and give the prediction with highest probability\n",
    "    best_pred, best_prob = 0, 0\n",
    "    batched_data = []\n",
    "    for w in range(conv1kernel, len(data), conv1stride):\n",
    "        x = np.array(data[max(0, w-window_size):w])\n",
    "        x = np.concatenate((np.zeros((window_size - x.shape[0], 6)), x))\n",
    "        x = np.float32(x)/4096\n",
    "        batched_data.append(x)\n",
    "    \n",
    "    batched_data = np.array(batched_data)\n",
    "    y = model.predict(batched_data)\n",
    "\n",
    "    # find the earliest prediction with probability greater than threshold\n",
    "    prediction_ended = False\n",
    "    for j in range(len(y)):\n",
    "        if np.max(y[j]) > highest_false_confidence and np.argmax(y[j]) != np.argmax(test_y_eval[i]):\n",
    "            highest_false_confidence = np.max(y[j])\n",
    "        if np.max(y[j]) > best_prob and not prediction_ended:\n",
    "            best_pred = np.argmax(y[j])\n",
    "            best_prob = np.max(y[j])\n",
    "        if np.max(y[j]) > threshold and not prediction_ended:\n",
    "            print(\"prediction ended early at window \" + str(j))\n",
    "            prediction_ended = True\n",
    "    if (not prediction_ended):\n",
    "        print(\"prediction ended at END OF SEQUENCE\")\n",
    "\n",
    "    print(\"best prob: \", str(best_prob), \"w correct predict\" if best_pred == np.argmax(test_y_eval[i]) else \"w wrong predict\")\n",
    "    pred.append(best_pred)\n",
    "    actual.append(np.argmax(test_y_eval[i]))\n",
    "result = confusion_matrix(actual, pred)\n",
    "sns.heatmap(result, annot=True, fmt=\"d\", xticklabels=action_types, yticklabels=action_types)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "# print accuracy\n",
    "print(\"Accuracy: \" + str(np.sum(np.diag(result))/np.sum(result)))\n",
    "\n",
    "# print highest false confidence\n",
    "print(\"Highest false confidence: \" + str(highest_false_confidence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"window shape: {window_size, data_depth}\")\n",
    "print(f\"kernel shape: {conv1kernel, data_depth}\")\n",
    "print(f\"first layer weights shape: {model.layers[0].get_weights()[0].shape}\")\n",
    "print(f\"first layer output shape: {model.layers[0].output_shape}\")\n",
    "print(f\"second layer weights shape: {model.layers[3].get_weights()[0].shape}\")\n",
    "print(f\"second layer output shape: {model.layers[3].output_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out weights and biases one by one\n",
    "layers_indexes = [0, 2]\n",
    "\n",
    "for layer_index in layers_indexes:\n",
    "    layer = model.layers[layer_index]\n",
    "    layer_name = layer.name\n",
    "    weights = layer.get_weights()\n",
    "    print(weights[0].shape)\n",
    "    # print(f\"INPUT_DTYPE model_param_{layer_name}_weights\")\n",
    "    # for i in range(weights[0].shape[-1]):\n",
    "    #     print(\"index\", i)\n",
    "    #     print(np.transpose(np.transpose(weights[0])[i]))\n",
    "    # print(f\"INPUT_DTYPE model_param_{layer_name}_biases\")\n",
    "    print(weights[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directly print out weights and biases\n",
    "# layers_indexes = [0, 3]\n",
    "\n",
    "for layer_index in layers_indexes:\n",
    "    print(model.layers[layer_index].name)\n",
    "\n",
    "for layer_index in layers_indexes:\n",
    "    layer = model.layers[layer_index]\n",
    "    layer_name = layer.name\n",
    "    weights = layer.get_weights()\n",
    "    if layer_index == 0:\n",
    "        layer_name = \"CNN\"\n",
    "        weights_size_definition = \"[CNN_KERNEL_LENGTH][CNN_KERNEL_DEPTH][CNN_KERNEL_COUNT]\"\n",
    "        bias_size_definition = \"[CNN_KERNEL_COUNT]\"\n",
    "    else:\n",
    "        layer_name = \"dense\"\n",
    "        weights_size_definition = \"[DENSE_INPUT_NODES][DENSE_OUTPUT_NODES]\"\n",
    "        bias_size_definition = \"[DENSE_OUTPUT_NODES]\"\n",
    "    print(f\"static CNN_DTYPE {layer_name}_weights{weights_size_definition} = {{\" + \", \".join([str(x) for x in weights[0].reshape(-1)]) + \"};\")\n",
    "    print(f\"static CNN_DTYPE {layer_name}_bias{bias_size_definition} = {{\" + \", \".join([str(x) for x in weights[1]]) + \"};\")\n",
    "\n",
    "    # save weights and biases to file\n",
    "    np.save(f\"{layer_name}_weights.npy\", weights[0])\n",
    "    np.save(f\"{layer_name}_bias.npy\", weights[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directly print out test dataset\n",
    "n_values_per_line = 10\n",
    "# dataset_size = len(test_x)\n",
    "dataset_size = 4\n",
    "dataset_start_index = 0\n",
    "print(\"#define DATASET_SIZE\", dataset_size)\n",
    "\n",
    "# print out float test dataset\n",
    "print(f\"const float test_x[DATASET_SIZE][INPUT_LENGTH][INPUT_DEPTH] = {{\")\n",
    "for text_x_index in range(dataset_start_index, min(test_x.shape[0], dataset_start_index + dataset_size)):\n",
    "    for datapoint_index in range(0, len(test_x[text_x_index])):\n",
    "        for i in range(0, len(test_x[text_x_index][datapoint_index]), n_values_per_line):\n",
    "            print(\", \".join([str(x) for x in test_x[text_x_index][datapoint_index][i:i+n_values_per_line]]) + \",\")\n",
    "print(\"};\") \n",
    "    \n",
    "print(f\"const int test_y[DATASET_SIZE][DENSE_OUTPUT_NODES] = {{\")\n",
    "for text_x_index in range(dataset_start_index, min(test_x.shape[0], dataset_start_index + dataset_size)):\n",
    "    for i in range(0, len(test_y[text_x_index]), n_values_per_line):\n",
    "        print(\", \".join([str(int(x)) for x in test_y[text_x_index][i:i+n_values_per_line]]) + \",\")\n",
    "print(\"};\")\n",
    "\n",
    "# # save the test dataset\n",
    "np.save(\"test_x.npy\", test_x_copy)\n",
    "np.save(\"test_y.npy\", test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directly print out integer test dataset\n",
    "n_values_per_line = 10\n",
    "dataset_size = len(test_x)\n",
    "# dataset_size = 10\n",
    "dataset_start_index = 0\n",
    "print(\"#define DATASET_SIZE\", dataset_size)\n",
    "\n",
    "# print out float test dataset\n",
    "print(f\"const float test_x[DATASET_SIZE][INPUT_LENGTH][INPUT_DEPTH] = {{\")\n",
    "for text_x_index in range(dataset_start_index, min(test_x.shape[0], dataset_start_index + dataset_size)):\n",
    "    for datapoint_index in range(0, len(test_x[text_x_index])):\n",
    "        for i in range(0, len(test_x[text_x_index][datapoint_index]), n_values_per_line):\n",
    "            print(\", \".join([str(x) for x in test_x_copy[text_x_index][datapoint_index][i:i+n_values_per_line]]) + \",\")\n",
    "print(\"};\") \n",
    "    \n",
    "print(f\"const int test_y[DATASET_SIZE][DENSE_OUTPUT_NODES] = {{\")\n",
    "for text_x_index in range(dataset_start_index, min(test_x.shape[0], dataset_start_index + dataset_size)):\n",
    "    for i in range(0, len(test_y[text_x_index]), n_values_per_line):\n",
    "        print(\", \".join([str(int(x)) for x in test_y[text_x_index][i:i+n_values_per_line]]) + \",\")\n",
    "print(\"};\")\n",
    "\n",
    "# save the test dataset\n",
    "np.save(\"test_x.npy\", test_x_copy)\n",
    "np.save(\"test_y.npy\", test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x[0].reshape((1, 75, 6)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print model output without the last layer\n",
    "from keras.models import Model\n",
    "model_cnn_output = Model(inputs=model.input, outputs=model.layers[0].output)\n",
    "model_cnn_averaged_output = Model(inputs=model.input, outputs=model.layers[2].output)\n",
    "model_raw_output = Model(inputs=model.input, outputs=model.layers[3].output)\n",
    "cnn_result = model_cnn_output.predict(test_x[1].reshape((1, 75, 6)))\n",
    "cnn_average = model_cnn_averaged_output.predict(test_x[1].reshape((1, 75, 6)))\n",
    "raw_result = model_raw_output.predict(test_x[1].reshape((1, 75, 6)))\n",
    "\n",
    "# print(\"CNN result:\")\n",
    "# for i in range(0, cnn_result.shape[1]):\n",
    "#     print(cnn_result[0][i])\n",
    "\n",
    "print(\"CNN average:\")\n",
    "# print in a single line\n",
    "print(\", \".join([str(x) for x in cnn_average[0]]))\n",
    "print(\"Raw result:\")\n",
    "print(\", \".join([str(x) for x in raw_result[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print model output without the last layer\n",
    "from keras.models import Model\n",
    "model_cnn_output = Model(inputs=model.input, outputs=model.layers[0].output)\n",
    "model_cnn_averaged_output = Model(inputs=model.input, outputs=model.layers[2].output)\n",
    "model_raw_output = Model(inputs=model.input, outputs=model.layers[3].output)\n",
    "cnn_result = model_cnn_output.predict(test_x)\n",
    "cnn_average = model_cnn_averaged_output.predict(test_x)\n",
    "raw_result = model_raw_output.predict(test_x)\n",
    "\n",
    "# print(\"CNN result:\")\n",
    "# for i in range(0, cnn_result.shape[1]):\n",
    "#     print(cnn_result[0][i])\n",
    "\n",
    "print(\"CNN average:\")\n",
    "for i in range(0, cnn_average.shape[0]):\n",
    "    print(\", \".join([str(x) for x in cnn_average[i]]))\n",
    "print(\"Raw result:\")\n",
    "for i in range(0, raw_result.shape[0]):\n",
    "    print(\", \".join([str(x) for x in raw_result[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('capstone')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38484b89ff4a5f0aa830a40dd0bba31da2c182e8f74971b909ccd23fb0e0e17f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
