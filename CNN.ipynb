{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout, Conv1D, MaxPooling1D, Softmax\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process dataset\n",
    "Extract data from the txt file for each user. We only extract the data for the time frame where the user is doing one action, not across different actions. Then we segment the data into 2 seconds window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file loader\n",
    "from os.path import isfile\n",
    "\n",
    "def load_data(file):\n",
    "    data = pd.read_csv(file, header=None)\n",
    "    return data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 75   # 50Hz, 75 samples = 1.5s of movement\n",
    "window_stride = 25\n",
    "action_types = ('shield', 'reload', 'grenade')\n",
    "data_depth = 6\n",
    "\n",
    "def load_dataset(data_dir):\n",
    "    dataset_x = []\n",
    "    dataset_y = []\n",
    "    for action_type in action_types:\n",
    "\n",
    "        for i in range(1, 100, 1):\n",
    "            # if file exists\n",
    "            if not isfile(data_dir + f\"/{action_type}{i}.csv\"):\n",
    "                break\n",
    "            data = load_data(data_dir + f\"/{action_type}{i}.csv\")\n",
    "\n",
    "            # sliding window\n",
    "            for j in range(0, len(data) - window_size+1, window_stride):\n",
    "                dataset_x.append(data[j:j + window_size])\n",
    "                dataset_y.append(action_types.index(action_type))\n",
    "\n",
    "    print(\"Dataset initialized with size: \" + str(len(dataset_y)))\n",
    "    for i in range(len(action_types)):\n",
    "        print(\"Class \" + str(i) + \" has \" + str(dataset_y.count(i)) + \" samples\")\n",
    "    dataset_x = np.int32(dataset_x)\n",
    "    dataset_y = to_categorical(dataset_y)\n",
    "    return np.array(dataset_x), np.array(dataset_y)\n",
    "\n",
    "# load dataset\n",
    "dataset_x, dataset_y = load_dataset(\"Dataset\")\n",
    "\n",
    "# split into train and test sets\n",
    "train_x, test_x, train_y, test_y = train_test_split(dataset_x, dataset_y, test_size=0.2, stratify = dataset_y, random_state=666)\n",
    "test_x_copy = copy.deepcopy(test_x)\n",
    "train_x, test_x = np.float32(train_x)/4096, np.float32(test_x)/4096\n",
    "# print(\"sample test x data: \" + str(test_x[0]))\n",
    "\n",
    "# summary of test dataset\n",
    "test = np.argmax(test_y, axis=1)\n",
    "print(\"\\nTest set distribution\")\n",
    "for i in range(len(action_types)):\n",
    "    print(\"Class \" + str(i) + \" has \" + str(test.tolist().count(i)) + \" samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1filters = 10\n",
    "conv1kernel = 15\n",
    "conv1stride = 5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(conv1filters, conv1kernel, strides=conv1stride, activation='relu', input_shape=(window_size, 6)))\n",
    "model.add(Dropout(0.5)) # 50% dropout\n",
    "# model.add(Conv1D(32, 40, activation='relu', data_format='channels_first'))\n",
    "model.add(Flatten())\n",
    "# model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(len(action_types)))\n",
    "model.add(Softmax())\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# performance before training\n",
    "y_prediction = np.argmax(model.predict(test_x), axis=1)\n",
    "result = confusion_matrix(np.argmax(test_y, axis=1), y_prediction)\n",
    "sns.heatmap(result, annot=True, fmt=\"d\", xticklabels=action_types, yticklabels=action_types)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint callback\n",
    "checkpoint_filepath = \"model_checkpoint/\"\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "\n",
    "# learning rate reduce on plateau callback\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=0)\n",
    "\n",
    "# early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def train_network(model, train_x, train_y, test_x, test_y):\n",
    "    verbose = 1 # 0 for no logging to stdout, 1 for progress bar logging, 2 for one log line per epoch.\n",
    "    epochs = 100\n",
    "    batch_size = 32\n",
    "    model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, validation_data=(test_x, test_y), callbacks = [model_checkpoint_callback, reduce_lr, early_stopping], verbose=verbose)\n",
    "    _, accuracy = model.evaluate(test_x, test_y, batch_size=batch_size, verbose=0)\n",
    "    return model\n",
    "\n",
    "model = train_network(model, train_x, train_y, test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draw confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.load_weights(checkpoint_filepath)\n",
    "y_prediction = np.argmax(model.predict(test_x), axis=1)\n",
    "result = confusion_matrix(np.argmax(test_y, axis=1), y_prediction)\n",
    "sns.heatmap(result, annot=True, fmt=\"d\", xticklabels=action_types, yticklabels=action_types)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"window shape: {window_size, data_depth}\")\n",
    "print(f\"kernel shape: {conv1kernel, data_depth}\")\n",
    "print(f\"first layer weights shape: {model.layers[0].get_weights()[0].shape}\")\n",
    "print(f\"first layer output shape: {model.layers[0].output_shape}\")\n",
    "print(f\"second layer weights shape: {model.layers[3].get_weights()[0].shape}\")\n",
    "print(f\"second layer output shape: {model.layers[3].output_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out weights and biases one by one\n",
    "layers_indexes = [0, 3]\n",
    "\n",
    "for layer_index in layers_indexes:\n",
    "    layer = model.layers[layer_index]\n",
    "    layer_name = layer.name\n",
    "    weights = layer.get_weights()\n",
    "    print(weights[0].shape)\n",
    "    print(f\"INPUT_DTYPE model_param_{layer_name}_weights\")\n",
    "    for i in range(weights[0].shape[-1]):\n",
    "        print(\"index\", i)\n",
    "        print(np.transpose(np.transpose(weights[0])[i]))\n",
    "    print(f\"INPUT_DTYPE model_param_{layer_name}_biases\")\n",
    "    print(weights[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directly print out weights and biases\n",
    "layers_indexes = [0, 3]\n",
    "\n",
    "for layer_index in layers_indexes:\n",
    "    layer = model.layers[layer_index]\n",
    "    layer_name = layer.name\n",
    "    weights = layer.get_weights()\n",
    "    if layer_index == 0:\n",
    "        layer_name = \"CNN\"\n",
    "        weights_size_definition = \"[CNN_KERNEL_LENGTH][CNN_KERNEL_DEPTH][CNN_KERNEL_COUNT]\"\n",
    "        bias_size_definition = \"[CNN_KERNEL_COUNT]\"\n",
    "    else:\n",
    "        layer_name = \"dense\"\n",
    "        weights_size_definition = \"[DENSE_INPUT_NODES][DENSE_OUTPUT_NODES]\"\n",
    "        bias_size_definition = \"[DENSE_OUTPUT_NODES]\"\n",
    "    print(f\"static INPUT_DTYPE {layer_name}_weights{weights_size_definition} = {{\" + \", \".join([str(x) for x in weights[0].reshape(-1)]) + \"};\")\n",
    "    print(f\"static INPUT_DTYPE {layer_name}_bias{bias_size_definition} = {{\" + \", \".join([str(x) for x in weights[1]]) + \"};\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directly print out test dataset\n",
    "n_values_per_line = 10\n",
    "dataset_size = len(test_x)\n",
    "# dataset_size = 1\n",
    "dataset_start_index = 0\n",
    "print(\"#define DATASET_SIZE\", dataset_size)\n",
    "\n",
    "# print out float test dataset\n",
    "print(f\"const float test_x[DATASET_SIZE][INPUT_LENGTH][INPUT_DEPTH] = {{\")\n",
    "for text_x_index in range(dataset_start_index, min(test_x.shape[0], dataset_start_index + dataset_size)):\n",
    "    for datapoint_index in range(0, len(test_x[text_x_index])):\n",
    "        for i in range(0, len(test_x[text_x_index][datapoint_index]), n_values_per_line):\n",
    "            print(\", \".join([str(x) for x in test_x_copy[text_x_index][datapoint_index][i:i+n_values_per_line]]) + \",\")\n",
    "print(\"};\") \n",
    "    \n",
    "print(f\"const int test_y[DATASET_SIZE][DENSE_OUTPUT_NODES] = {{\")\n",
    "for text_x_index in range(dataset_start_index, min(test_x.shape[0], dataset_start_index + dataset_size)):\n",
    "    for i in range(0, len(test_y[text_x_index]), n_values_per_line):\n",
    "        print(\", \".join([str(int(x)) for x in test_y[text_x_index][i:i+n_values_per_line]]) + \",\")\n",
    "print(\"};\")\n",
    "\n",
    "# save the test dataset\n",
    "np.save(\"test_x.npy\", test_x_copy)\n",
    "np.save(\"test_y.npy\", test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print model output without the last layer\n",
    "from keras.models import Model\n",
    "model_without_last_layer = Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "model_without_last_layer.summary()\n",
    "predicted_result = model_without_last_layer.predict(test_x)[0]\n",
    "print(predicted_result.shape)\n",
    "print(predicted_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('capstone')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f82d4ec2f4e1a7949fc551b5039d8b80c5fc6c2b366144cfac1fa6211cdc80ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
